# q_learning

В ході лабораторної роботи було розроблено розумного робота,
що змозі досягти цілі за найменшу кількість кроків.
Був вивчений та застосований алгоритм Q навчання.
Спершу бот блукає, оскільки не знає маршрут до цілі.
В основному він спочатку орієнтується на таблицю R -таблиця доступних рухів.
З кожною епохою бот все детальніше запам’ятовує дорогу.
Його пам’ять це таблиця Q й з кожною епохою бот все частіше користується саме нею для наближення до цілі.

Ця таблиця заповнюється згідно формули оновлення таблиці Q:
Q[s, a] = R[s, a] + Gamma * MAX(Q[s', a']),
де Q[s, a] – комірка матриці Q, що відповідає поточному стану агента;
R[s, a] – комірка матриці R, що відповідає поточному стану агента;
Gamma – швидкість навчання, рекомендоване значення = 0.8;
Q[s', a'] – комірка матриці Q, що відповідає наступному стану агента;
MAX(Q[s', a']) – вибір з множини можливих дій агента в поточному
стані дії з максимальною винагородою.
